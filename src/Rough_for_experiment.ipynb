{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6e8c05",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2675791054.py, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    clustered_idxs.append( np.concatenate( i*num_users_per_cluster : ( i + 1 ) * num_users_per_cluster ) )\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = args_parser()\n",
    "    exp_details(args)\n",
    "\n",
    "    # if args.gpu_id:\n",
    "        # torch.cuda.set_device(args.gpu_id)\n",
    "    # device = 'cuda' if args.gpu else 'cpu'\n",
    "    device = 'cpu'\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "\n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                               dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    # print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 2\n",
    "    val_loss_pre, counter = 0, 0\n",
    "\n",
    "    # Users clustering\n",
    "    num_clusters = 25\n",
    "    num_users_per_cluster = int( args.num_user / num_clusters )\n",
    "    clustered_idxs = []\n",
    "    for i in range( num_clusters ):\n",
    "        clustered_idxs.append( np.concatenate( i*num_users_per_cluster : ( i + 1 ) * num_users_per_cluster ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            # local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "            #                           idxs=user_groups[idx], logger=logger)\n",
    "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                      idxs=clustered_idxs[idx], logger=logger)\n",
    "            w, loss = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        # update global weights\n",
    "        global_weights = average_weights(local_weights)\n",
    "\n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # Calculate avg training accuracy over all users at every epoch\n",
    "        list_acc, list_loss = [], []\n",
    "        global_model.eval()\n",
    "        for c in range(args.num_users):\n",
    "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                      idxs=user_groups[idx], logger=logger)\n",
    "            acc, loss = local_model.inference(model=global_model)\n",
    "            list_acc.append(acc)\n",
    "            list_loss.append(loss)\n",
    "        train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "    print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "    # Saving the objects train_loss and train_accuracy:\n",
    "    file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "        format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
    "               args.local_ep, args.local_bs)\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "\n",
    "    # PLOTTING (optional)\n",
    "    # import matplotlib\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # matplotlib.use('Agg')\n",
    "\n",
    "    # Plot Loss curve\n",
    "    # plt.figure()\n",
    "    # plt.title('Training Loss vs Communication rounds')\n",
    "    # plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "    # plt.ylabel('Training loss')\n",
    "    # plt.xlabel('Communication Rounds')\n",
    "    # plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "    #                    args.iid, args.local_ep, args.local_bs))\n",
    "    #\n",
    "    # # Plot Average Accuracy vs Communication rounds\n",
    "    # plt.figure()\n",
    "    # plt.title('Average Accuracy vs Communication rounds')\n",
    "    # plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "    # plt.ylabel('Average Accuracy')\n",
    "    # plt.xlabel('Communication Rounds')\n",
    "    # plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "    #             format(args.dataset, args.model, args.epochs, args.frac,\n",
    "    #                    args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549241a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935aff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569470a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb07414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4]\n",
      "[1, 2, 3, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,3,4]\n",
    "b = list(a)\n",
    "print(b)\n",
    "c = list(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4330f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4, [1, 2, 3, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "c.append(list(b))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce5acb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ab9100",
   "metadata": {},
   "outputs": [],
   "source": [
    " clustered_idxs = {i: np.array([]) for i in range(num_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e9e92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([], dtype=float64), 1: array([], dtype=float64), 2: array([], dtype=float64), 3: array([], dtype=float64), 4: array([], dtype=float64), 5: array([], dtype=float64), 6: array([], dtype=float64), 7: array([], dtype=float64), 8: array([], dtype=float64), 9: array([], dtype=float64), 10: array([], dtype=float64), 11: array([], dtype=float64), 12: array([], dtype=float64), 13: array([], dtype=float64), 14: array([], dtype=float64), 15: array([], dtype=float64), 16: array([], dtype=float64), 17: array([], dtype=float64), 18: array([], dtype=float64), 19: array([], dtype=float64)}\n"
     ]
    }
   ],
   "source": [
    "print(clustered_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "517cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( num_clusters ):\n",
    "        # temp = []\n",
    "#         for j in range( num_users_per_cluster ):\n",
    "\n",
    "            clustered_idxs[i] = np.concatenate( ( clustered_idxs[i], a ) , axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0c95752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 3., 4.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8005e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "a = set([1,2,3,5])\n",
    "b = set([2,3,4])\n",
    "d[0] = a\n",
    "d[1] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04ad8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7ff1a1531c40>\n"
     ]
    }
   ],
   "source": [
    "print(int(i) for i in d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a85bd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([i for i in range(90)])\n",
    "num_users = 5\n",
    "num_items = int(len(dataset)/num_users)\n",
    "dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "for i in range(num_users):\n",
    "    dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "    all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "# return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9eaeaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [int(i) for i in dict_users[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a57e8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 73, 42, 11, 75, 44, 78, 79, 48, 81, 14, 43, 21, 55, 88, 58, 61, 62]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5ab75e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d29c675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.union(np.array([1,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cdd9350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "if a == 2:\n",
    "    print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c215c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ae4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice( range( 100 ), size = ( 10, 10 ), replace = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7897b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 25 72 93 54 27  1  3 35 49]\n",
      " [46 36 76 26 42 66 24 87 14 94]\n",
      " [82 58 37 65 73 96 21 28 10  9]\n",
      " [22 79 92 88 60 62 29 43 52 44]\n",
      " [63 91 98 16 13 15  5 80 11  2]\n",
      " [70 57 97 50 38 99 32 53 90 39]\n",
      " [75 74 19 40 34 45 83 30  6 48]\n",
      " [67 47 20 78 71  7 64 31 41 81]\n",
      " [18 56 68 95 89  0 33  4 85 17]\n",
      " [59 84 69 86 61 12 77 23 55 51]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca6d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(10, size = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f5a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 2 8]\n",
      " [6 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a24bc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.choice(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73a0b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7702b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set()\n",
    "a = a.union([2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2caf754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d172d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(a), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ff804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
